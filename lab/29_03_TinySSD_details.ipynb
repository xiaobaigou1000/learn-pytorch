{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "0cf46169",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "import PIL.Image\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas\n",
    "import pytorch_lightning as L\n",
    "from pathlib import Path\n",
    "from torchvision.transforms import v2\n",
    "from torch.utils import data\n",
    "from matplotlib.patches import Rectangle\n",
    "from neural_networks.tinyssd import TinySSD\n",
    "from lightning_datasets import BananaDetection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb711fc9",
   "metadata": {},
   "source": [
    "When process predict for TinySSD model, anchors are not affected by input batch size.\n",
    "\n",
    "In output, `cls_preds` and `bbox_preds` contains predicts for each anchor, position is same with anchor index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "913ffdd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When input 4 image\n",
      "    anchors_shape: torch.Size([1, 5444, 4])\n",
      "    cls_preds_shape: torch.Size([4, 5444, 2])\n",
      "    bbox_preds_shape: torch.Size([4, 21776])\n",
      "\n",
      "When input 8 image\n",
      "    anchors_shape: torch.Size([1, 5444, 4])\n",
      "    cls_preds_shape: torch.Size([8, 5444, 2])\n",
      "    bbox_preds_shape: torch.Size([8, 21776])\n",
      "\n",
      "When input 12 image\n",
      "    anchors_shape: torch.Size([1, 5444, 4])\n",
      "    cls_preds_shape: torch.Size([12, 5444, 2])\n",
      "    bbox_preds_shape: torch.Size([12, 21776])\n",
      "\n",
      "When input 16 image\n",
      "    anchors_shape: torch.Size([1, 5444, 4])\n",
      "    cls_preds_shape: torch.Size([16, 5444, 2])\n",
      "    bbox_preds_shape: torch.Size([16, 21776])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = TinySSD(1)\n",
    "\n",
    "for i in range(4, 17, 4):\n",
    "    X = torch.zeros(i, 3, 256, 256)\n",
    "    anchors, cls_preds, bbox_preds = model(X)\n",
    "    print(\n",
    "        f\"When input {i} image\\n\"\n",
    "        f\"    anchors_shape: {anchors.shape}\\n\"\n",
    "        f\"    cls_preds_shape: {cls_preds.shape}\\n\"\n",
    "        f\"    bbox_preds_shape: {bbox_preds.shape}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7b6789",
   "metadata": {},
   "source": [
    "In every batch, we will have raw data with following shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "2490f298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Raw data shape in batch idx 0\n",
      "    X.shape: torch.Size([32, 3, 256, 256])\n",
      "    y.shape: torch.Size([32, 1, 5])\n",
      "    anchors.shape: torch.Size([1, 5444, 4])\n",
      "    cls_preds.shape: torch.Size([32, 5444, 2])\n",
      "    bbox_preds.shape: torch.Size([32, 21776])\n"
     ]
    }
   ],
   "source": [
    "dataset = BananaDetection(batch_size=32, num_workers=0)\n",
    "dataset.prepare_data()\n",
    "dataset.setup('fit')\n",
    "\n",
    "for batch in dataset.train_dataloader():\n",
    "    X, y = batch\n",
    "    anchors, cls_preds, bbox_preds = model(X)\n",
    "    print(f\"\"\"\n",
    "Raw data shape in batch idx 0\n",
    "    X.shape: {X.shape}\n",
    "    y.shape: {y.shape}\n",
    "    anchors.shape: {anchors.shape}\n",
    "    cls_preds.shape: {cls_preds.shape}\n",
    "    bbox_preds.shape: {bbox_preds.shape}\"\"\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "e2a1f019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.7500, 0.0234, 0.9375, 0.2578]], dtype=torch.float64)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee46d8f5",
   "metadata": {},
   "source": [
    "The dim 0 for anchors was added in `TinySSD.multibox_prior` function, seems can be removed.\n",
    "\n",
    "To keep consistent with d2l book, keep it in code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "f4b4aa4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_iou(boxes1, boxes2):\n",
    "    box_area = lambda boxes: ((boxes[:, 2] - boxes[:, 0]) *\n",
    "    (boxes[:, 3] - boxes[:, 1]))\n",
    "    # boxes1,boxes2,areas1,areas2的形状:\n",
    "    # boxes1：(boxes1的数量,4),\n",
    "    # boxes2：(boxes2的数量,4),\n",
    "    # areas1：(boxes1的数量,),\n",
    "    # areas2：(boxes2的数量,)\n",
    "    areas1 = box_area(boxes1)\n",
    "    areas2 = box_area(boxes2)\n",
    "    # inter_upperlefts,inter_lowerrights,inters的形状:\n",
    "    # (boxes1的数量,boxes2的数量,2)\n",
    "    inter_upperlefts = torch.max(boxes1[:, None, :2], boxes2[:, :2])\n",
    "    inter_lowerrights = torch.min(boxes1[:, None, 2:], boxes2[:, 2:])\n",
    "    inters = (inter_lowerrights - inter_upperlefts).clamp(min=0)\n",
    "    # inter_areasandunion_areas的形状:(boxes1的数量,boxes2的数量)\n",
    "    inter_areas = inters[:, :, 0] * inters[:, :, 1]\n",
    "    union_areas = areas1[:, None] + areas2 - inter_areas\n",
    "    return inter_areas / union_areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "9bdfbc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_anchor_to_bbox(ground_truth, anchors, iou_threshold=0.5):\n",
    "    num_anchors = anchors.shape[0]\n",
    "    num_gt_boxes = ground_truth.shape[0]\n",
    "    jaccard = box_iou(anchors, ground_truth)\n",
    "    anchors_bbox_map = torch.full((num_anchors,), -1, dtype=torch.long)\n",
    "    print(f\"box_iou result:\\n{jaccard}\")\n",
    "    # 这里算法的顺序和书上写的略有差异，实现顺序和逻辑顺序相反\n",
    "    # 为了方便计算，先根据iou_threshold把每个锚框都打上标号，然后再用最适合的锚框覆盖一部分值\n",
    "    max_ious, indices = torch.max(jaccard, dim=1)\n",
    "    anchors_bbox_map[max_ious >=\n",
    "                     iou_threshold] = indices[max_ious >= iou_threshold]\n",
    "    # 然后开始按照iou最大值，给每个真实边框，分配最接近的锚框\n",
    "    column_discard_placeholder = torch.full((num_anchors,), -1)\n",
    "    row_discard_placeholder = torch.full((num_gt_boxes,), -1)\n",
    "\n",
    "    for _ in range(num_gt_boxes):\n",
    "        max_idx = torch.argmax(jaccard)\n",
    "        box_idx = (max_idx % num_gt_boxes).long()\n",
    "        anc_idx = (max_idx / num_gt_boxes).long()\n",
    "        anchors_bbox_map[anc_idx] = box_idx\n",
    "        jaccard[anc_idx, :] = row_discard_placeholder\n",
    "        jaccard[:, box_idx] = column_discard_placeholder\n",
    "    return anchors_bbox_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "eef79069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "box_iou result:\n",
      "tensor([[0.0536, 0.0000],\n",
      "        [0.1417, 0.0000],\n",
      "        [0.0000, 0.5657],\n",
      "        [0.0000, 0.2059],\n",
      "        [0.0000, 0.7459]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-1,  0,  1, -1,  1])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth = torch.tensor([[0, 0.1, 0.08, 0.52, 0.92],\n",
    "                             [1, 0.55, 0.2, 0.9, 0.88]])\n",
    "anchors = torch.tensor([[0, 0.1, 0.2, 0.3], [0.15, 0.2, 0.4, 0.4],\n",
    "                        [0.63, 0.05, 0.88, 0.98], [0.66, 0.45, 0.8, 0.8],\n",
    "                        [0.57, 0.3, 0.92, 0.9]])\n",
    "\n",
    "assign_anchor_to_bbox(ground_truth[:,1:],anchors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "7a634b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_corner_to_center(boxes):\n",
    "    \"\"\"从（左上，右下）转换到（中间，宽度，高度）\"\"\"\n",
    "    x1, y1, x2, y2 = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]\n",
    "    cx = (x1 + x2) / 2\n",
    "    cy = (y1 + y2) / 2\n",
    "    w = x2 - x1\n",
    "    h = y2 - y1\n",
    "    boxes = torch.stack((cx, cy, w, h), axis=-1)\n",
    "    return boxes\n",
    "\n",
    "\n",
    "def box_center_to_corner(boxes):\n",
    "    \"\"\"从（中间，宽度，高度）转换到（左上，右下）\"\"\"\n",
    "    cx, cy, w, h = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]\n",
    "    x1 = cx - 0.5 * w\n",
    "    y1 = cy - 0.5 * h\n",
    "    x2 = cx + 0.5 * w\n",
    "    y2 = cy + 0.5 * h\n",
    "    boxes = torch.stack((x1, y1, x2, y2), axis=-1)\n",
    "    return boxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "afedcd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def offset_boxes(anchors, assigned_bb, eps=1e-6):\n",
    "    \"\"\"对锚框偏移量的转换\"\"\"\n",
    "    c_anc = box_corner_to_center(anchors)\n",
    "    c_assigned_bb = box_corner_to_center(assigned_bb)\n",
    "    offset_xy = 10 * (c_assigned_bb[:, :2] - c_anc[:, :2]) / c_anc[:, 2:]\n",
    "    offset_wh = 5 * torch.log(eps + c_assigned_bb[:, 2:] / c_anc[:, 2:])\n",
    "    offset = torch.cat([offset_xy, offset_wh], axis=1)\n",
    "    return offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "4bab2807",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multibox_target(anchors, labels):\n",
    "    batch_size, anchors = labels.shape[0], anchors.squeeze(0)\n",
    "\n",
    "    num_anchors = anchors.shape[0]\n",
    "\n",
    "    batch_offset, batch_mask, batch_class_labels = [], [], []\n",
    "\n",
    "    for image_idx in range(batch_size):\n",
    "        label = labels[image_idx]\n",
    "        anchors_bbox_map = assign_anchor_to_bbox(label[:, 1:], anchors)\n",
    "        # bbox_mask is used to mask backgroud as 0.\n",
    "        # label other than background will multiply an identity of 1\n",
    "        # bbox_mask.shape[0] equals num_anchors\n",
    "        bbox_mask = (anchors_bbox_map >= 0).float().unsqueeze(-1).repeat(1, 4)\n",
    "\n",
    "        class_labels = torch.zeros(num_anchors, dtype=torch.long)\n",
    "        assigned_bb = torch.zeros((num_anchors, 4), dtype=torch.float32)\n",
    "\n",
    "        indices_true = torch.nonzero(anchors_bbox_map >= 0)\n",
    "        bounding_box_idx = anchors_bbox_map[indices_true]\n",
    "\n",
    "        class_labels[indices_true] = label[bounding_box_idx, 0].long() + 1\n",
    "        assigned_bb[indices_true] = label[bounding_box_idx, 1:]\n",
    "        offset = offset_boxes(anchors, assigned_bb) * bbox_mask\n",
    "        batch_offset.append(offset.reshape(-1))\n",
    "        batch_mask.append(bbox_mask.reshape(-1))\n",
    "        batch_class_labels.append(class_labels)\n",
    "\n",
    "    bbox_offset = torch.stack(batch_offset)\n",
    "    bbox_mask = torch.stack(batch_mask)\n",
    "    class_labels = torch.stack(batch_class_labels)\n",
    "\n",
    "    return bbox_offset, bbox_mask, class_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be612666",
   "metadata": {},
   "source": [
    "These works are used to construct labels used to train models.\n",
    "\n",
    "The output can match model output, which meas the target we want neural networks to generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "d4897597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "box_iou result:\n",
      "tensor([[0.0536, 0.0000],\n",
      "        [0.1417, 0.0000],\n",
      "        [0.0000, 0.5657],\n",
      "        [0.0000, 0.2059],\n",
      "        [0.0000, 0.7459]])\n"
     ]
    }
   ],
   "source": [
    "model_train_label = multibox_target(anchors.unsqueeze(0), ground_truth.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "91b473d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label used to train model:\n",
      "    bounding box offset(match bounding box predict output): tensor([[-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,  1.4000e+00,\n",
      "          1.0000e+01,  2.5940e+00,  7.1754e+00, -1.2000e+00,  2.6882e-01,\n",
      "          1.6824e+00, -1.5655e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "         -0.0000e+00, -5.7143e-01, -1.0000e+00,  4.1723e-06,  6.2582e-01]])\n",
      "    bounding box mask(0 means background, 1 meas object): tensor([[0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1.,\n",
      "         1., 1.]])\n",
      "    class labels(match class predict output): tensor([[0, 1, 2, 0, 2]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"\n",
    "Label used to train model:\n",
    "    bounding box offset(match bounding box predict output): {model_train_label[0]}\n",
    "    bounding box mask(0 means background, 1 meas object): {model_train_label[1]}\n",
    "    class labels(match class predict output): {model_train_label[2]}\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
